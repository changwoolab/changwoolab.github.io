---
layout: post
title: "CS285 lec2: Imitation Learning"
date: 2024-10-20 13:55:29 +0900
categories: cs285
---

## Terminology & Notation

<img src="/public/img/28-1.png" style="display: block; margin: auto;" width="600" />

## Notion of Imitation Learning

<img src="/public/img/28-2.png" style="display: block; margin: auto;" width="600" />

**Problem**

- Imitation learning via behavioral cloning **is not guaranteed to work!**
    - Because **assumption of independent data does not hold** (Supervised learning assumes that each data is independent)

## Distributional Shift Problem (Why behavioral cloning fails)

### High-level understanding

Because $p_{\pi_\theta}$ is different from $p_{\text{data}}$, mistakes are accumulated, which leads to something very different from what we wanted

<img src="/public/img/28-3.png" style="display: block; margin: auto;" width="600" />

### Mathematical Analysis

- Analysis
    
    **What we want:** 
    
    $c(s_t, a_t) = \begin{cases}0 & \text{if } a_t = \pi^*(s_t) \\1 & \text{otherwise}\end{cases}$
    
    We want to **minimize the number of mistakes**: Minimize $E_{s_t \sim p_{\pi_\theta}(s_t)}[c(s_t,a_t)]$
    
    **Analysis**
    
    $\epsilon$ = Maximum probability of making mistake.
    
    Assume $\pi_\theta(a\ne \pi^*(s)|s) \le \epsilon$ for $s \sim p_{\text{train}}(s)$
    
    if $p_{\text{train}}(s) \ne p_\theta(s):$
    
    $p_\theta(s_t) = \underbrace{(1 - \epsilon)^t}_{\text{probability we made no mistakes}} p_{\text{train}}(s_t) + \left( 1 - (1 - \epsilon)^t \right) \underbrace{p_{\text{mistake}}(s_t)}_{\text{some other distribution}}$
    
    Then, the difference between $p_\text{train}$ and $p_\theta$ is
    
    $$
    \begin{equation}
    \begin{aligned}
    |p_\theta(s_t) - p_{\text{train}}(s_t)| &= (1 - (1 - \epsilon)^t) |p_{\text{mistake}}(s_t) - p_{\text{train}}(s_t)| \\
    &\leq 2(1 - (1 - \epsilon)^t) \\
    &\leq 2\epsilon t \quad \because (1 - \epsilon)^t \geq 1 - \epsilon t \quad \text{for } \epsilon \in [0, 1]
    \end{aligned}
    \end{equation}
    $$
    
    Then, we can expand our cost function like below
    
    $$
    \begin{aligned}\sum_t \mathbb{E}_{p_\theta(s_t)}[c_t] &= \sum_t \sum_{s_t} p_\theta(s_t) c_t(s_t) \\&= \sum_t \sum_{s_t} \left(p_\theta(s_t) - p_{\text{train}}(s_t) + p_{\text{train}}(s_t)\right) c_t(s_t) \\
    &\leq \sum_t \sum_{s_t} p_{\text{train}}(s_t) c_t(s_t) + |p_\theta(s_t) - p_{\text{train}}(s_t)| c_{\text{max}} \\&\leq \sum_t \epsilon + 2 \epsilon t \\&= O(\epsilon T^2)\end{aligned}
    $$
    
    which means that as time goes on, the mistakes are accumulated quadratically, and it leads to the distribution shift!
    

## Addressing Distributional Shift Problem

### Data Augmentation

We can **intentionally add mistakes and corrections** → **Data Augmentation!**

Add some “fake” data that illustrates corrections

### Use Model that makes very few mistakes

Why does robot fail to fit the human?

1. **Non-Markovian behavior**
    
    In markovian setting $\pi_\theta(a_t|o_t)$, if we see the same situation, we do the exactly same thing regardless of what happend before! → Very unnatural for human!
    
    **Human behavior depends on all past observations**! $\pi_\theta(a_t|o_1,...,o_t)$
    
    - How to remember and use history? → Sequence Model
        
        <img src="/public/img/28-4.png" style="display: block; margin: auto;" width="600" />
        
        - Limitations: Model can **confuse the cause of the action**
            - EX) Model can learn wrong like below
            “slow down” → “pressing brake” not “pressing brake” → “slow down”
2. **Multimodal behavior**
    
    Within same state, multiple action choices could be selected → model cannot act well
    
    **Solutions**
    
    - **Expressive continuous distributions**
        - Mixture of Gaussians
            
            <img src="/public/img/28-5.png" style="display: block; margin: auto;" width="600" />
            
            limitations
            
            - limited to the number of predefined dimensions
            - It is inefficient when the dimension goes higher and higher
        - Latent variable models
            
            <img src="/public/img/28-6.png" style="display: block; margin: auto;" width="600" />
            
        - Diffusion models
            
            <img src="/public/img/28-7.png" style="display: block; margin: auto;" width="600" />
            
            <img src="/public/img/28-8.png" style="display: block; margin: auto;" width="600" />
            
    - **Discretization with high-dimensional action spaces**
        - Autoregressive discretization (discretize 1 dimension at a time)
            
            <img src="/public/img/28-9.png" style="display: block; margin: auto;" width="600" />
            

### Multi-task Learning

**Why is learning many tasks easier?**

The **policy $\pi_\theta(a|s,p)$ is conditioned on the task $p$** and learns to perform multiple tasks, sharing information across these tasks. Because tasks have **shared core skills**, agent can develop more generalized representations.

**Goal-conditioned behavioral cloning**

1. **Training data** consist of sequences of states $(s_1,a_1,...,s_T,a_T)$ that represent a demonstration of how to reach a goal $s_T$
2. **Goal-conditioned policy $\pi_\theta(a|s,g)$**
    
    Agent learns a policy that is conditioned on both the state $s$ and goal $g$
    
3. **Benefit**: Instead of learning separate policies for every possible goal (traditional), the policy is **goal-conditioned**, which enables it to handle multiple goals efficiently.

### DAgger (Dataset Aggregation)

This algorithm thinks in reverse! Instead of fitting $p_{\theta_\pi} → p_\text{data}$, be clever about $p_\text{data}$!

**Goal: Collect training data from $p_{\pi_\theta}(o_t)$ instead of $p_\text{data}(o_t)$**

**How?: Just run $\pi_\theta(a_t|o_t)$ and label them!**

1. Train $\pi_\theta(a_t|o_t)$ from human data $\mathcal{D}=\{o_1,a_1,...,o_N,a_N\}$
2. Run $\pi_\theta(a_t|o_t)$ to get dataset $\mathcal{D}=\{o_1,...,o_M\}$
3. Ask human to label $\mathcal{D}_\pi$ with actions $a_t$
4. Aggregate: $\mathcal{D} \gets \mathcal{D} \cup \mathcal{D}_\pi$ 

## Problem of imitation learning

- Finite dataset
- Humans are not good at providing some kinds of actions
- Humans can autonomously → Can’t robot do that?
