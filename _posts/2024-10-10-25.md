---
layout: post
title: "Paper Review 24: A Neural Probabilistic Language Model"
date: 2024-10-10 13:55:29 +0900
categories: paper-review
---


## Summary

The model learns both **(1) a distributed word representation** (where each word is associated with a real-valued vector), and **(2) a probability function over word sequences based on these representations**. 

This allows the model to generalize from a training sentence to a large number of semantically similar sentences, **overcoming the curse of dimensionality** by leveraging word similarity.

## **1. Motivation and Idea**

The fundamental problem of statistical language modeling, which learns the probability distribution of word sequences in a language, lies in the **curse of dimensionality**—when dealing with discrete random variables (such as words in a sentence), the potential number of different combinations is immense. This makes it hard for models to generalize well to new sequences that haven’t been seen during training.

Traditional n-gram models, which base predictions on the probabilities of short sequences of words (usually 2 to 5 words), address this challenge but struggle with unseen word combinations. The authors propose to overcome this by using **distributed word representations**—vector embeddings of words—and modeling the probability function over sequences with a neural network. This allows the model to generalize from the training data to similar word sequences by sharing information between semantically or syntactically related words.

## **2. Fighting the Curse of Dimensionality with Distributed Representations**

The model combats the curse of dimensionality by:

1. **Distributed Word Representation**: Each word in the vocabulary is associated with a **real-valued vector** in a high-dimensional space. These vectors represent different features of the words, capturing semantic and syntactic properties.
2. **Joint Probability Function**: The model is expressed by the probability of a word sequence in terms of the feature vectors of the words in the sequence. A neural network is used to predict the next word based on the feature vectors of the preceding words.
3. **Simultaneous Learning**: The model learns the distributed word vectors and the parameters of the probability function simultaneously, making it more efficient at generalizing to unseen data.

**The model will so generalize** because similar words are expected to have a similar feature vector, and because the probability function is a smooth function of these feature vectors, a small change in the features will induce a small change in the probability. 

## **3. Related Work**

The key difference between this model and earlier attempts, such as **class-based n-grams**, is that instead of clustering words into discrete classes, the model uses continuous word vectors to represent word similarities. The neural network approach is also compared to other methods like Latent Semantic Indexing (LSI), which is used in information retrieval to learn word representations based on document co-occurrences. However, the authors argue that learning representations specific to language modeling is more useful for tasks like word prediction.

## **4. A Neural Model**

This section explains the architecture of the proposed neural model in detail:

<img src="/public/img/25.png" style="display: block; margin: auto;" width="100%" />

- **Vocabulary Representation**: Each word in the vocabulary is mapped to a distributed word feature vector, stored in a matrix where each row corresponds to a word’s vector.
- **Conditional Probability Function**: The neural network uses these word vectors to predict the probability of the next word given the previous words. The network can have hidden layers and a **softmax output layer** to normalize the output probabilities over all words in the vocabulary.
- **Perplexity**: The model’s performance is measured using **perplexity**, a standard metric for language models, which is the exponential of the average negative log-likelihood. Lower perplexity indicates better predictive performance.

## **5. Experimental Results**

This section presents the results of experiments conducted on two corpora: the **Brown corpus** and the **Associated Press News** corpus. The authors compare the performance of their neural network model against state-of-the-art **n-gram models** and **class-based n-grams**.

Key findings include:

- The neural model significantly outperformed the traditional n-gram models, with perplexity reductions of about **24%** on the Brown corpus and **8%** on the AP News corpus.
- The model was able to take advantage of longer word contexts (e.g., up to 5 previous words), which was a limitation of traditional n-grams.
- Combining the neural model with an interpolated trigram model further improved performance, indicating that the two models make complementary errors.